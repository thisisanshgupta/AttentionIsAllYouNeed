{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AttentionIsAllYouNeed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Attention is All you Need\n",
        "![](https://devopedia.org/images/article/235/8482.1573652874.png)"
      ],
      "metadata": {
        "id": "qO97wrS2Iy_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "LnSHDQBNHaAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEvK6kckD9wf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "EXkhh61AGg34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,vocab_size,embed_dim,max_len,num_layers,num_heads,ff_hid_dim,dropout=0.5):\n",
        "       super().__init__()\n",
        "       self.tok_embed = nn.Embedding(vocab_size,embed_dim)\n",
        "       self.pos_embed = nn.Embedding(max_len,embed_dim)\n",
        "       self.layers = nn.ModuleList([\n",
        "          EncoderLayer(embed_dim,num_heads,ff_hid_dim,dropout)\n",
        "          for _ in range(num_layers)\n",
        "       ])\n",
        "       self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,src,src_mask):\n",
        "       batch_size = src.size(0)\n",
        "       seq_len = sex.size(1)\n",
        "       device = next(self.parameters()).device\n",
        "       pos = (\n",
        "           torch.arange(0, seq_len)\n",
        "           .unsqueeze(0)\n",
        "           .repeat(batch_size, 1)\n",
        "           .to(device)\n",
        "       )\n",
        "       src = self.dropout(self.pos_embed(pos) + self.tok_embed(src))\n",
        "       for layer in self.layers:\n",
        "           src = layer(src,src_mask)\n",
        "       return src"
      ],
      "metadata": {
        "id": "Yd0fV9kWEXEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EncoderLayer"
      ],
      "metadata": {
        "id": "y3dNzSevGhx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "   def __init__(self,embed_dim,num_heads,ff_hid_dim,dropout):\n",
        "      super().__init__()\n",
        "      self.ff_ln = nn.LayerNorm(embed_dim)\n",
        "      self.attention_ln = nn.LayerNorm(embed_dim)\n",
        "      self.ff = FeedForward(embed_dim, ff_hid_dim, dropout)\n",
        "      self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "   def forward(self, src, src_mask):\n",
        "       attention_out = self.attention(src, src, src, src_mask)\n",
        "       attetion_ln_out = self.dropout(self.attention_ln(src + attention_out))\n",
        "       ff_out = self.ff(attetion_ln_out)\n",
        "       ff_ln_out = self.dropout(self.ff_ln(attetion_ln_out + ff_out))\n",
        "       return ff_ln_out"
      ],
      "metadata": {
        "id": "lgXpwMzgFk8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHeadAttention"
      ],
      "metadata": {
        "id": "fUvraV2YHVfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,hid_dim,num_heads):\n",
        "        super().__init__()\n",
        "        assert hid_dim % num_heads == 0, \"`hidden_dim` must be a multiple of `num_heads`\"\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hid_dim // num_heads\n",
        "        \n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim, bias=False) #value\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim, bias=False) #key\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim, bias=False) #query\n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "    \n",
        "    def forward(self, value, key, query, mask=None):\n",
        "        # keys.shape = [batch_size, seq_len, embed_dim]\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        V = self.fc_v(value)\n",
        "        K = self.fc_k(key)\n",
        "        Q = self.fc_q(query)\n",
        "        # shape = [batch_size, seq_len, hid_dim]\n",
        "        \n",
        "        V = V.reshape(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K_t = K.reshape(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 3, 1)\n",
        "        Q = Q.reshape(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V.shape = [batch_size, num_heads, value_len, head_dim]\n",
        "        # K_t.shape = [batch_size, num_heads, head_dim, key_len]\n",
        "        \n",
        "        energy = torch.matmul(Q, K_t) / (self.hid_dim ** 1/2)\n",
        "        # energy.shape = [batch_size, num_heads, query_len, key_len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        \n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        weighted = torch.matmul(attention, V)\n",
        "        # weighted.shape = [batch_size, num_heads, seq_len, head_dim]\n",
        "        weighted = weighted.permute(0, 2, 1, 3)\n",
        "        # weighted.shape = [batch_size, seq_len, num_heads, head_dim]\n",
        "        weighted = weighted.reshape(batch_size, -1, self.hid_dim)\n",
        "        # weighted.shape = [batch_size, seq_len, hid_dim]\n",
        "        out = self.fc(weighted)\n",
        "        # out.shape = [batch_size, seq_len, hid_dim]\n",
        "        return out"
      ],
      "metadata": {
        "id": "XRu7RyHxHLzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FeedForward Layer"
      ],
      "metadata": {
        "id": "igSyoWHFIdgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hid_dim, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hid_dim)\n",
        "        self.fc2 = nn.Linear(hid_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.dropout(self.fc1(x))))"
      ],
      "metadata": {
        "id": "ERxVyMuDHYgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "chF7Ej09JRDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embed_dim,\n",
        "        max_len,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        ff_hid_dim,\n",
        "        dropout=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderLayer(embed_dim, num_heads, ff_hid_dim, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, trg, trg_mask, enc_src, src_mask):\n",
        "        batch_size = trg.size(0)\n",
        "        seq_len = trg.size(1)\n",
        "        device = next(model.parameters()).device\n",
        "        pos = (\n",
        "            torch.arange(0, seq_len)\n",
        "            .unsqueeze(0)\n",
        "            .repeat(batch_size, 1)\n",
        "            .to(device)\n",
        "        )\n",
        "        trg = self.dropout(self.pos_embed(pos) + self.tok_embed(trg))\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, trg_mask, enc_src, src_mask)\n",
        "        out = self.fc(trg)\n",
        "        return out"
      ],
      "metadata": {
        "id": "VXkfQS9LJF9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DecoderLayer"
      ],
      "metadata": {
        "id": "XdTJaT_MJa_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 embed_dim, \n",
        "                 num_heads, \n",
        "                 ff_hid_dim, \n",
        "                 dropout\n",
        "                ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.ff_ln = nn.LayerNorm(embed_dim)\n",
        "        self.dec_attn_ln = nn.LayerNorm(embed_dim)\n",
        "        self.enc_attn_ln = nn.LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_hid_dim, dropout)\n",
        "        self.dec_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.enc_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, trg_mask, enc_src, src_mask):\n",
        "        dec_attn_out = self.dropout(self.dec_attn(trg, trg, trg, trg_mask))\n",
        "        dec_attn_ln_out = self.dec_attn_ln(trg + dec_attn_out)\n",
        "        enc_attn_out = self.dropout(\n",
        "            self.enc_attn(enc_src, enc_src, dec_attn_ln_out, src_mask)\n",
        "        )\n",
        "        enc_attn_ln_out = self.enc_attn_ln(dec_attn_ln_out + enc_attn_out)\n",
        "        ff_out = self.dropout(self.ff(enc_attn_ln_out))\n",
        "        ff_ln_out = self.ff_ln(ff_out + enc_attn_ln_out)\n",
        "        return ff_ln_out"
      ],
      "metadata": {
        "id": "2HxhwdxWJTp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "aLGkD4EGKBUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        src_vocab_size, \n",
        "        trg_vocab_size,\n",
        "        src_pad_idx, \n",
        "        trg_pad_idx,\n",
        "        embed_dim=512,\n",
        "        max_len=100,\n",
        "        num_layers=12,\n",
        "        num_heads=8,\n",
        "        ff_hid_dim=2048,\n",
        "        dropout=0.5,\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(\n",
        "            src_vocab_size,\n",
        "            embed_dim,\n",
        "            max_len,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            ff_hid_dim,\n",
        "            dropout,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            trg_vocab_size,\n",
        "            embed_dim,\n",
        "            max_len,\n",
        "            num_layers,\n",
        "            num_heads,\n",
        "            ff_hid_dim,\n",
        "            dropout,\n",
        "        )\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "    \n",
        "    def make_src_mask(self, src):\n",
        "        # src.shape = [batch_size, src_len]\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # src.shape = [batch_size, 1, 1, src_len]\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        batch_size = trg.size(0)\n",
        "        seq_len = trg.size(1)\n",
        "        pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        seq_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "        trg_mask = pad_mask * seq_mask\n",
        "        return trg_mask\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        device = next(model.parameters()).device\n",
        "        src_mask = self.make_src_mask(src).to(device)\n",
        "        trg_mask = self.make_trg_mask(trg).to(device)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        decoder_out = self.decoder(trg, trg_mask, enc_src, src_mask)\n",
        "        return decoder_out"
      ],
      "metadata": {
        "id": "rJpntrxeKEYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main method"
      ],
      "metadata": {
        "id": "BbUezb3vKV9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "src = torch.tensor(\n",
        "    [[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]\n",
        ").to(device)\n",
        "trg = torch.tensor([[1, 7, 4, 3, 5, 0, 0, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(\n",
        "    device\n",
        ")"
      ],
      "metadata": {
        "id": "5zBmeWiwKQzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(\n",
        "    src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx\n",
        ").to(device)\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68qSRcZOKYOE",
        "outputId": "ffedcd10-5a64-424f-e310-bc379bd5efe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embed): Embedding(10, 512)\n",
              "    (pos_embed): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (6): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (7): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (8): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (9): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (10): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (11): EncoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (attention): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embed): Embedding(10, 512)\n",
              "    (pos_embed): Embedding(100, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (3): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (4): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (5): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (6): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (7): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (8): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (9): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (10): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "      (11): DecoderLayer(\n",
              "        (ff_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dec_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff): FeedForward(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.5, inplace=False)\n",
              "        )\n",
              "        (dec_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (enc_attn): MultiHeadAttention(\n",
              "          (fc_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-yc_uaGULLpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}